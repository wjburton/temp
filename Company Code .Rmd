---
title: "BrightBytes Data Challenge"
output:
  html_notebook: default
  html_document: default
---

###Intro
**Objective**: Use this dataset to help understand what factors are the best predictors of postsecondary enrollment, and offer suggestions to operationalize those insights to help Brightbytes.

**Interpretation**: which factors are the best predictors of gaining a further education after highschool. This includes any additional including college/university and associates degreed. 

**Methodology**: Make the target binary. 1 = Post secondary (Only Bachelors for this exercise), 0 = non-post secondary (Indicated by a -7). Identify factors that are associated with higher rates of post secondary education.
Ignore variables that are asking questions about why you attended 
One key question to think about when identifying relationships: Is it the variable that makes a student more likely to go to college, or is it the type of student they are that makes them do the variable, and get a postsecondary education. 
<br>
<br>

Load libraries and data
```{r, warning = FALSE}
library(tidyverse)
df <- read_csv('/home/will/link_to_wjburton/Documents/Professional Info/Company Code Challenges/BrightBytes/HSLS_2009_v3_0/students.csv')
```

Quick data summary
```{r}
df %>% data.frame() %>% str()
df %>% summary()
df %>% head()
df %>% tail()
```
All variables are in fact integers/numeric. One issue noticed is that there are many negative values. After a quick scan on the website I found these are: 

• −5 = “Data Suppressed”—indicates values that are available on the restricted-use data
but suppressed on the public-use data.
• −7 = “Item legitimate skip/NA”—indicates items that are programmatically skipped
based on rules in the questionnaire and are not applicable to those respondents.
• −8 = “Nonrespondent/component NA”—indicates that data are not available because
of unit nonresponse or the interview component did not apply (e.g., student has no
mathematics class, thus the mathematics teacher interview does not apply).
• −9 = “Missing”—indicates item level missing where the question may apply to the
respondent but it is not answered, or the question is not administered because the
gate/introductory question is not answered.



Build modeling dataset:
Based on Data dictionary, only keep variables that would be useful for building predictive models, and rename columns to be more interpretable. (Keep variables that are known before the post secondary decision is made)
In this dataset I only keep rows containing S3PROGLEVEL having values of -7 and 1. It is then converted to binary with 1 = post secondary (bachelors), and -7 = not post secondary
```{r}
table(df$S3PROGLEVEL)
df <- df %>% 
      select(-one_of('X1','S3FALLHSID', 'X3STATE', 'SCH_ID', 'S3ENROLLHS13', 'S3FOCUS','S3CLGFT',
                     'S3WORKFT', 'S3CLGLVL', 'S3CLGSTATE', 'S3AAB4BA', 'S3BATRANSFER', 'S3REPUTATION',
                     'S3COSTATTEND','S3DISTANCE',
                     'S3CNSLCLG', 'S3CNSLAID', 'S3CNSLJOB', 'S3JOBPLC', 'S34YRBAPLC', 'S3SPORTS', 
                     'S3SOCIALLIFE', 'S3ONLINE', 'S3NODEBT', 'S3CANAFFORD',
                     'S3INELIGIBLE', 'S3DKHOW', 'S3DKCOULD', 'S3NOPOSTSEC', 'S3FAMNOTQUAL', 
                     'S3DONOTWANT', 'S3NOCLGOTHRSN', 'S3WHYNOTCLG', 'STU_ID')) %>% 
      rename(parent_occ = X1PAR1OCC_STEM1, socio_econ = X1SES, fafsa = S3APPFAFSA,
             location = X3LOCALE, alg1 = X3T1CREDALG1, alg2 = X3T1CREDALG2, int_math = X3T1CREDINTM,
             pre_calc = X3T1CREDPREC, stat = X3T1CREDSTAT, when_alg1 = X3TWHENALG1,
             ap_class = S3AP, ib_class = S3IB, col_class = S3DUAL, ap_math = S3APMATH,
             ap_sci = S3APSCIENCE, ap_other = S3APOTHER, ib_math = S3IBMATH, ib_sci = S3IBSCIENCE,
             ib_other = S3IBOTHER, col_math = S3DUALMATH, col_sci = S3DUALSCIENCE, 
             col_other = S3DUALOTHER, col_influence = S3CLGINFLU,
             post_second = S3PROGLEVEL) %>% 
      filter(post_second == -7 | post_second == 1) %>% 
      mutate(post_second  = ifelse(post_second == -7,0,1)) %>% 
      mutate_if(is.integer, as.factor)

set.seed(10)
train_samp <- sample(1:nrow(df), size = 0.9 * nrow(df))
train_df <- df[train_samp,] 
test_df <- df[-train_samp,]
```

Instead of exploring all variables and associations, lets use a random forest to pull out the variables that could be of the most useful for predicting post secondary education.
```{r}
forest <- randomForest::randomForest(factor(post_second) ~ ., train_df)
randomForest::varImpPlot(forest)
```

Based on the output, it appears as expected: FAFSA is the top followed by socio economic status. I'll look at some of the other top rated variables, and ignore some of the others for this exercise.

##Exploration
Explore variable reationships with post secondary
FAFSA - Don't include this in a predictive model, but it could be useful as a recommendation. FAFSA only applies to students thinking about a further education so it isn't a qualified variable to use for prediction.
```{r}
train_df %>% 
  group_by(fafsa) %>% 
  summarise(pct_post_second = sum(post_second)/n(),
            n = n()) %>% 
  arrange(pct_post_second) %>%
  mutate(fafsa = factor(fafsa,levels = unique(fafsa))) %>%
  ggplot(aes(fafsa, pct_post_second)) + geom_bar(aes(alpha = log(n)),stat = 'identity') + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



socio-econ
```{r}
#Missing ~ 1000 socio economic values, but this is an incredibly strong predictor. I'll just add #an indicator if it is = -8
hist(train_df$socio_econ)
plot_continuous(train_df$socio_econ[train_df$socio_econ > -8],  n = 100,
                response = train_df$post_second[train_df$socio_econ > -8], max_poly_degree = 1, log_odds = F, variable_name = 'Socio Economic')
```

ap_class
```{r}
table(train_df$ap_class, train_df$post_second)
train_df %>% 
  group_by(ap_class) %>% 
  summarise(pct_post_second = sum(post_second)/n(),
            n = n()) %>% 
  arrange(pct_post_second) %>%
  mutate(ap_class = factor(ap_class,levels = unique(ap_class))) %>%
  ggplot(aes(ap_class, pct_post_second)) + geom_bar(aes(alpha = n),stat = 'identity') + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
#combine negative
```

college influence
```{r}
table(train_df$col_influence, train_df$post_second)
train_df %>% 
  group_by(col_influence) %>% 
  summarise(pct_post_second = sum(post_second)/n(),
            n = n()) %>% 
  arrange(pct_post_second) %>%
  mutate(col_influence = factor(col_influence,levels = unique(col_influence))) %>%
  ggplot(aes(col_influence, pct_post_second)) + geom_bar(aes(alpha = n),stat = 'identity') + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
#col_influence: combine -9 & -4, school_mentor = 3 & 1, interactions = 12 & 7 & 6 & 11 & 5, fam = 9 & 4 & 2 & 10
```
"ib_class"     
```{r}
table(train_df$ib_class, train_df$post_second)
train_df %>% 
  group_by(ib_class) %>% 
  summarise(pct_post_second = sum(post_second)/n(),
            n = n()) %>% 
  arrange(pct_post_second) %>%
  mutate(ib_class = factor(ib_class,levels = unique(ib_class))) %>%
  ggplot(aes(ib_class, pct_post_second)) + geom_bar(aes(alpha = n),stat = 'identity') + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
#combine negative
```

col_class
```{r}
table(train_df$col_class, train_df$post_second)
train_df %>% 
  group_by(col_class) %>% 
  summarise(pct_post_second = sum(post_second)/n(),
            n = n()) %>% 
  filter(n > 30) %>% 
  arrange(pct_post_second) %>%
  mutate(col_class = factor(col_class,levels = unique(col_class))) %>%
  ggplot(aes(col_class, pct_post_second)) + geom_bar(aes(alpha = n),stat = 'identity') + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
#col_class combine negatives
```

Parent occupation
```{r}
train_df %>% 
  group_by(parent_occ) %>% 
  summarise(pct_post_second = sum(post_second)/n(),
            n = n()) %>% 
  arrange(pct_post_second) %>%
  mutate(parent_occ = factor(parent_occ,levels = unique(parent_occ))) %>%
  ggplot(aes(parent_occ, pct_post_second)) + geom_bar(aes(alpha = log(n)),stat = 'identity') + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

#combine negatives
```

Location
```{r}
table(train_df$location, train_df$post_second)
train_df %>% 
  group_by(location) %>% 
  summarise(pct_post_second = sum(post_second)/n(),
            n = n()) %>% 
  arrange(pct_post_second) %>%
  mutate(location = factor(location,levels = unique(location))) %>%
  ggplot(aes(location, pct_post_second)) + geom_bar(aes(alpha = n),stat = 'identity') + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
#location: combine negatives, 3 & 4, 1 & 2
```


### Final dataset manipulation
```{r}
#hand select variables to use in the model
transform_data <- function(df, train = T){
  mod_df <- df %>%
            select(parent_occ, socio_econ, location, ib_class, 
            ap_class, col_class, col_influence, post_second) %>% 
            mutate_if(is.factor, funs(as.integer(as.character(.)))) %>% 
            mutate_if(is.integer, funs(ifelse(as.integer(.) < 0, '_miss',.))) %>% 
            mutate_if(is.character, funs(as.factor)) 
            
  mod_df <- model.matrix(~., data = mod_df) %>% data.frame() %>%  .[,-1]
  
  if(train == T){
    keep_vars <- mod_df %>% apply(2,sum) %>% .[.>50] %>% names %>% c('socio_econ')
  }else{
    keep_vars = names(mod_df)
  }
  
  mod_df <- mod_df %>% 
            select(keep_vars) %>% 
            mutate(socio_missing = ifelse(socio_econ == -8,1,0),
                   non_ib_ap_col = ifelse(ap_class_miss == 1 & ib_class_miss == 1 & col_class_miss == 1,1,0))
}

clean_train_df <- transform_data(train_df)
clean_test_df <- transform_data(test_df, train = F)
```



### Modeling

I'm choosing to build a logistic regression model so I can interpret the association with the response
```{r}
#Forward selection:
#define full logistic model
full_logistic_mod <- glm(post_second ~ ., data = clean_train_df, family = binomial)

#define empty model
nothing <- glm(post_second ~ 1, data = clean_train_df, family = binomial)

#preform forwards, selection, while optimizing AIC
forwards <- step(nothing,
                 scope=list(lower=formula(nothing),upper=formula(full_logistic_mod)), direction="forward", trace = 0)

#check if variables selected were too similar
selected_vars <- clean_train_df %>% select(forwards$coefficients %>% names() %>% .[-1])
cor(selected_vars)
#remove ap_class_miss, keep non_ib_ap_col

# I'll drop the remaining variables that aren't statistically significant
final_model <- glm(post_second ~ col_influence4 + 
                   ap_class3 + socio_econ + socio_missing + ap_class2 + col_influence8 + 
                   parent_occ_miss + location_miss + col_influence11 + col_influence6 + 
                   col_influence12 + parent_occ4 + location4 + location3 + col_class2 + 
                   col_class3 + ib_class3 + col_influence9 + col_influence_miss + non_ib_ap_col,
                   data = clean_train_df, family = binomial)



```


##Model testing
```{r}
preds <- predict(final_model, clean_test_df)
```





















Additional Topics that aren't explored here:
What Factors are important to Post Secondary Goers (could translate to action):
S3ONLINE, S3SOCIALLIFE, S3SPORTS, S3JOBPLC, S3DISTANCE, S3COSTATTEND, S3REPUTATION

What Factors attribute to not going to post secondary (could translate to action):
S3WHYNOTCLG, S3NOCLGOTHRSN, S3DONOTWANT

FAFSA Exploration (could translate to aciton:
S3APPFAFSA, S3NODEBT, S3CANAFFORD, S3INELIGIBLE, S3DKHOW, S3DKCOULD,
S3NOPOSTSEC, S3FAMNOTQUAL








Ideas: Parents: respondants to the survey who don't know shows either 1. It is survey data and the respondents didn't care, or 2. they aren't very involved with their child's life. The 2nd may be actionable.






Functions I had built previously to visualize relationships between a continuous predictor and 
a binary response
```{r}
#' Calculate the ROC curve for a given model
#' @description This function calculates the roc curve and returns the data in a data frame
#' @param model  model of interest
#' @param response  response variable
#' @param name  name of model
#' @param probs predicted probabilities
#' @export
calculate_ROC <- function(model = NULL, response, name, probs = NULL){
  df <- NULL
  tp_rates <- NULL
  fp_rates <- NULL
  probs <- if(is.null(probs)){predict(model, type= 'response')} else{probs}
  AUC <- as.numeric(pROC::auc(response, probs))
  for(threshold in 0:200){
    preds <- ifelse(probs >= (threshold/200), 1,0)
    confusion_matrix <- caret::confusionMatrix(preds, response)$table
    POS <- confusion_matrix[2,2]
    NEG <- confusion_matrix[1,1]
    FALPOS <- confusion_matrix[2,1]
    FALNEG <- confusion_matrix[1,2]
    tp_rate <- POS / (POS + FALNEG)
    fp_rate <- FALPOS / (NEG + FALPOS)
    tn_rate <- NEG / (NEG + FALPOS)
    SPECIFICITY  <- tn_rate
    SENSIT <- tp_rate
    M1SPEC <- 1 - SPECIFICITY
    df <- rbind(df, data.frame(name, AUC,'PROB' = threshold/200,
                               POS, NEG, FALPOS, FALNEG, SENSIT,
                               M1SPEC, youden_index = (SENSIT + SPECIFICITY - 1),
                               accuracy = (POS + NEG)/(POS + NEG + FALPOS + FALNEG)))

  }
 
  return(df)
}


#'Create bins containing equal sample sizes spread across a variable
#'@description Create bins containing equal sample sizes. If the density is not
#'uniform then generic cuts in the data will not produce equal sample sizes among
#'the cuts.
#'@param variable  Variable of interest
#'@param n  The sample size for each bin
create_equal_bins <- function(variable, n){
  place <- 1:length(variable)
  n_bins <- ceiling(length(variable)/n)
  binned_var <- rep(1:n_bins,n) %>% sort %>% .[1:length(variable)]
  
  data.frame(place, variable) %>%
    arrange(variable) %>%
    mutate(binned_var = binned_var) %>%
    group_by(binned_var) %>%
    mutate(point = median(variable)) %>%
    ungroup() %>%
    arrange(place) -> out
  
  return(out)
}

#'Plot a continuous variable against a binary response
#'@description Plot a continuous variable against a binary response by
#'binning the variables and looking at the proportion of the response in each bin
#'@param variable  A binned version of the continuous variable
#'@param int An interaction term to add to the model. Can be used to fit more complicated relationships
#'@param n The number of datapoints for each condensed point
#'@param response Binary response variable
#'@param out Output the data used to create the plot or not. Accepts values of TRUE or FALSE
#'@param max_poly_degree The highest degree of polynomial to use for modeling
#'@param variable_name Name of the variable, Will be the title of the x-axis
#'@param jitter_height The random jitter of the actual points. the default may be too large or small
#'@param log_odds Should the y-axis be in terms of log odds or probability. T = log odds, F = Probability
#'@export
plot_continuous <- function(variable, int = NULL, n = 30, response, out = F,
                            max_poly_degree = 1, variable_name = 'variable',
                            jitter_height = 0.1, log_odds = T){

  df <- create_equal_bins(variable, n)
  df <- data.frame(df, response) %>% na.omit()
  
  df %>%
  group_by(point) %>%
  summarise(prob_warm = sum(response)/n(),
            n = n()) %>%
  mutate(odds = prob_warm/(1-prob_warm)) %>%
  mutate(log_odds = log(odds)) -> association_tbl

  if(is.null(int)){
    log_mod <- glm(response ~ poly(variable,max_poly_degree) ,data = df, family = "binomial") } else {
       log_mod <- glm(response ~ poly(variable,max_poly_degree) + int ,data = df, family = "binomial")
    }
  
  probs <- predict(log_mod , type = 'response')
  df$pred_prob <- probs
  df$pred_log_odds <- log(probs/(1-probs))

  max_log_odds <- max(association_tbl$log_odds[!is.infinite(association_tbl$log_odds)])
  min_log_odds <- min(association_tbl$log_odds[!is.infinite(association_tbl$log_odds)])
  add <- max(abs(association_tbl$log_odds[!is.infinite(association_tbl$log_odds)]))


  if(log_odds == T){
    df$plot_point <- ifelse(df$response == 1, max_log_odds + .1*add,
                            min_log_odds - .1*add)

    print(ggplot()+ geom_point(aes(x = association_tbl$point, y = association_tbl$log_odds)) +
            geom_line(aes(x = df$variable, y = df$pred_log_odds))+
            geom_jitter(aes(x = df$variable, y = df$plot_point),
                           color = 'blue', alpha = .05, height = jitter_height, width = 0) +
            xlab(variable_name) + ylab('log odds of making over 50k'))
  } else{
    df$plot_point <- ifelse(df$response == 1,max(df$pred_prob) + 0.15, min(df$pred_prob) - 0.15 )
    print(ggplot()+ geom_point(aes(x = association_tbl$point, y = association_tbl$prob_warm)) +
            geom_line(aes(x = df$variable, y = df$pred_prob))+
            geom_jitter(aes(x = df$variable, y = df$plot_point),
                        color = 'blue', alpha = .05, height = jitter_height, width = 0) +
            xlab(variable_name) + ylab('probability of making over 50k'))
  }

  if(out == T){
    return(list('points' = association_tbl, 'line' = df))
  }
}
  
  
  
#' Plot overlaying density plots of the variable against the response
#' @description Identify separation created by the independent variable on the response
#' @param variable independent variable
#' @param response response variable
#' @param variable_name name of independent variable or x-axis title
#' @export
plot_density <- function(variable, response, variable_name = 'variable'){
    data.frame(variable, response) %>%
    mutate(response = factor(response)) %>%
    rename(over_50k = response) -> density_plot

    print(ggplot(density_plot, aes(x = variable, fill = over_50k)) + geom_density(alpha = 0.2)+
      xlab(variable_name) + scale_fill_manual(values=c("blue", "red")))
}

```
